{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thefuzz\n",
      "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
      "  Downloading rapidfuzz-3.12.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading rapidfuzz-3.12.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.6 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, thefuzz\n",
      "Successfully installed rapidfuzz-3.12.1 thefuzz-0.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ResumeChunker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 700,\n",
    "        chunk_overlap: int = 50,\n",
    "        embeddings_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def process_file(self, text: str) -> List[str]:\n",
    "        try:\n",
    "            # Initial splitting\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            logger.info(f\"Created {len(chunks)} initial chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "def process_and_save_chunks(input_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Process all files in the input folder and save chunks to output folder\n",
    "    \"\"\"\n",
    "    chunker = ResumeChunker()\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    logger.info(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    # List all files in input folder\n",
    "    input_files = os.listdir(input_folder)\n",
    "    logger.info(f\"Found {len(input_files)} files in input folder\")\n",
    "\n",
    "    all_chunks = []\n",
    "    processed_files = 0\n",
    "\n",
    "    for filename in input_files:\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        logger.info(f\"Processing file: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Read the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    logger.info(f\"Successfully loaded JSON from {filename}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    # If not JSON, try reading as plain text\n",
    "                    f.seek(0)\n",
    "                    data = {'content': f.read()}\n",
    "                    logger.info(f\"Loaded {filename} as plain text\")\n",
    "\n",
    "            # Extract content\n",
    "            content = data.get('content', '')\n",
    "            if not content:\n",
    "                logger.warning(f\"No content found in {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Process the content\n",
    "            chunks = chunker.process_file(content)\n",
    "            logger.info(f\"Created {len(chunks)} chunks for {filename}\")\n",
    "\n",
    "            # Save chunks for this file\n",
    "            chunks_with_metadata = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_data = {\n",
    "                    'original_file': filename,\n",
    "                    'chunk_id': f\"{filename}_chunk_{i}\",\n",
    "                    'content': chunk,\n",
    "                }\n",
    "                chunks_with_metadata.append(chunk_data)\n",
    "\n",
    "            # Save individual file chunks\n",
    "            output_path = os.path.join(output_folder, f\"{filename}_chunks.json\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunks_with_metadata, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"Saved chunks for {filename}\")\n",
    "\n",
    "            all_chunks.extend(chunks_with_metadata)\n",
    "            processed_files += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save all chunks to a single file\n",
    "    if all_chunks:\n",
    "        # Save as JSON\n",
    "        all_chunks_path = os.path.join(output_folder, \"all_chunks.json\")\n",
    "        with open(all_chunks_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved combined chunks to {all_chunks_path}\")\n",
    "\n",
    "        # Save as CSV\n",
    "        df = pd.DataFrame(all_chunks)\n",
    "        csv_path = os.path.join(output_folder, \"all_chunks.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"Saved chunks to CSV: {csv_path}\")\n",
    "\n",
    "    logger.info(f\"Processing complete. Processed {processed_files} files, created {len(all_chunks)} total chunks\")\n",
    "    return len(all_chunks)\n",
    "\n",
    "\n",
    "def test_single_file(file_path: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Test processing a single file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            logger.info(f\"Reading file: {file_path}\")\n",
    "            content = f.read()\n",
    "            logger.info(f\"File content length: {len(content)}\")\n",
    "\n",
    "        chunker = ResumeChunker()\n",
    "        chunks = chunker.process_file(content)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "        # Save test output\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        test_output_path = os.path.join(output_folder, \"test_chunks.json\")\n",
    "        with open(test_output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved test chunks to {test_output_path}\")\n",
    "\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in test: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 23 files in CVs\\output\n",
      "INFO:__main__:Testing with first file: CVs\\output\\Ali Mohamed Behery_CV.txt\n",
      "INFO:__main__:Reading file: CVs\\output\\Ali Mohamed Behery_CV.txt\n",
      "INFO:__main__:File content length: 4585\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Created 9 initial chunks\n",
      "INFO:__main__:Created 9 chunks\n",
      "INFO:__main__:Saved test chunks to CVs\\chunked_output\\test_chunks.json\n",
      "INFO:__main__:Processing all files...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Created output folder: CVs\\chunked_output\n",
      "INFO:__main__:Found 23 files in input folder\n",
      "INFO:__main__:Processing file: Ali Mohamed Behery_CV.txt\n",
      "INFO:__main__:Loaded Ali Mohamed Behery_CV.txt as plain text\n",
      "INFO:__main__:Created 9 initial chunks\n",
      "INFO:__main__:Created 9 chunks for Ali Mohamed Behery_CV.txt\n",
      "INFO:__main__:Saved chunks for Ali Mohamed Behery_CV.txt\n",
      "INFO:__main__:Processing file: Amr Elsayyad.txt\n",
      "INFO:__main__:Loaded Amr Elsayyad.txt as plain text\n",
      "INFO:__main__:Created 8 initial chunks\n",
      "INFO:__main__:Created 8 chunks for Amr Elsayyad.txt\n",
      "INFO:__main__:Saved chunks for Amr Elsayyad.txt\n",
      "INFO:__main__:Processing file: BusinessIntelligenceDeveloper_EnasHagras.txt\n",
      "INFO:__main__:Loaded BusinessIntelligenceDeveloper_EnasHagras.txt as plain text\n",
      "INFO:__main__:Created 5 initial chunks\n",
      "INFO:__main__:Created 5 chunks for BusinessIntelligenceDeveloper_EnasHagras.txt\n",
      "INFO:__main__:Saved chunks for BusinessIntelligenceDeveloper_EnasHagras.txt\n",
      "INFO:__main__:Processing file: DataEngineer_AhmedHassan.txt\n",
      "INFO:__main__:Loaded DataEngineer_AhmedHassan.txt as plain text\n",
      "INFO:__main__:Created 10 initial chunks\n",
      "INFO:__main__:Created 10 chunks for DataEngineer_AhmedHassan.txt\n",
      "INFO:__main__:Saved chunks for DataEngineer_AhmedHassan.txt\n",
      "INFO:__main__:Processing file: DataEngineer_MostafaKhalilKarrar.txt\n",
      "INFO:__main__:Loaded DataEngineer_MostafaKhalilKarrar.txt as plain text\n",
      "INFO:__main__:Created 19 initial chunks\n",
      "INFO:__main__:Created 19 chunks for DataEngineer_MostafaKhalilKarrar.txt\n",
      "INFO:__main__:Saved chunks for DataEngineer_MostafaKhalilKarrar.txt\n",
      "INFO:__main__:Processing file: DataScienceIntern_MariamTowfik.txt\n",
      "INFO:__main__:Loaded DataScienceIntern_MariamTowfik.txt as plain text\n",
      "INFO:__main__:Created 8 initial chunks\n",
      "INFO:__main__:Created 8 chunks for DataScienceIntern_MariamTowfik.txt\n",
      "INFO:__main__:Saved chunks for DataScienceIntern_MariamTowfik.txt\n",
      "INFO:__main__:Processing file: DataScienceIntern_YaraMostafa.txt\n",
      "INFO:__main__:Loaded DataScienceIntern_YaraMostafa.txt as plain text\n",
      "INFO:__main__:Created 5 initial chunks\n",
      "INFO:__main__:Created 5 chunks for DataScienceIntern_YaraMostafa.txt\n",
      "INFO:__main__:Saved chunks for DataScienceIntern_YaraMostafa.txt\n",
      "INFO:__main__:Processing file: DataScientist_AhmedEldamaty.txt\n",
      "INFO:__main__:Loaded DataScientist_AhmedEldamaty.txt as plain text\n",
      "INFO:__main__:Created 10 initial chunks\n",
      "INFO:__main__:Created 10 chunks for DataScientist_AhmedEldamaty.txt\n",
      "INFO:__main__:Saved chunks for DataScientist_AhmedEldamaty.txt\n",
      "INFO:__main__:Processing file: DataScientist_MohamedMaher.txt\n",
      "INFO:__main__:Loaded DataScientist_MohamedMaher.txt as plain text\n",
      "INFO:__main__:Created 16 initial chunks\n",
      "INFO:__main__:Created 16 chunks for DataScientist_MohamedMaher.txt\n",
      "INFO:__main__:Saved chunks for DataScientist_MohamedMaher.txt\n",
      "INFO:__main__:Processing file: DataScientist_MohamedMostafa.txt\n",
      "INFO:__main__:Loaded DataScientist_MohamedMostafa.txt as plain text\n",
      "INFO:__main__:Created 4 initial chunks\n",
      "INFO:__main__:Created 4 chunks for DataScientist_MohamedMostafa.txt\n",
      "INFO:__main__:Saved chunks for DataScientist_MohamedMostafa.txt\n",
      "INFO:__main__:Processing file: DataScientist_OsamaOthman.txt\n",
      "INFO:__main__:Loaded DataScientist_OsamaOthman.txt as plain text\n",
      "INFO:__main__:Created 1 initial chunks\n",
      "INFO:__main__:Created 1 chunks for DataScientist_OsamaOthman.txt\n",
      "INFO:__main__:Saved chunks for DataScientist_OsamaOthman.txt\n",
      "INFO:__main__:Processing file: DataTesting_ReemAyman.txt\n",
      "INFO:__main__:Loaded DataTesting_ReemAyman.txt as plain text\n",
      "INFO:__main__:Created 5 initial chunks\n",
      "INFO:__main__:Created 5 chunks for DataTesting_ReemAyman.txt\n",
      "INFO:__main__:Saved chunks for DataTesting_ReemAyman.txt\n",
      "INFO:__main__:Processing file: DataTesting_SondosAkram.txt\n",
      "INFO:__main__:Loaded DataTesting_SondosAkram.txt as plain text\n",
      "INFO:__main__:Created 8 initial chunks\n",
      "INFO:__main__:Created 8 chunks for DataTesting_SondosAkram.txt\n",
      "INFO:__main__:Saved chunks for DataTesting_SondosAkram.txt\n",
      "INFO:__main__:Processing file: Eman Ismail Muhammad Resume_SW_ML.txt\n",
      "INFO:__main__:Loaded Eman Ismail Muhammad Resume_SW_ML.txt as plain text\n",
      "INFO:__main__:Created 10 initial chunks\n",
      "INFO:__main__:Created 10 chunks for Eman Ismail Muhammad Resume_SW_ML.txt\n",
      "INFO:__main__:Saved chunks for Eman Ismail Muhammad Resume_SW_ML.txt\n",
      "INFO:__main__:Processing file: extraction_summary.csv\n",
      "INFO:__main__:Loaded extraction_summary.csv as plain text\n",
      "INFO:__main__:Created 206 initial chunks\n",
      "INFO:__main__:Created 206 chunks for extraction_summary.csv\n",
      "INFO:__main__:Saved chunks for extraction_summary.csv\n",
      "INFO:__main__:Processing file: Salma Mansour Hassan.txt\n",
      "INFO:__main__:Loaded Salma Mansour Hassan.txt as plain text\n",
      "INFO:__main__:Created 12 initial chunks\n",
      "INFO:__main__:Created 12 chunks for Salma Mansour Hassan.txt\n",
      "INFO:__main__:Saved chunks for Salma Mansour Hassan.txt\n",
      "INFO:__main__:Processing file: Salma Mansour.txt\n",
      "INFO:__main__:Loaded Salma Mansour.txt as plain text\n",
      "INFO:__main__:Created 15 initial chunks\n",
      "INFO:__main__:Created 15 chunks for Salma Mansour.txt\n",
      "INFO:__main__:Saved chunks for Salma Mansour.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_MahmoudHelmy.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_MahmoudHelmy.txt as plain text\n",
      "INFO:__main__:Created 15 initial chunks\n",
      "INFO:__main__:Created 15 chunks for SW_MLEngineer_MahmoudHelmy.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_MahmoudHelmy.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_MahmoudSaeed.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_MahmoudSaeed.txt as plain text\n",
      "INFO:__main__:Created 10 initial chunks\n",
      "INFO:__main__:Created 10 chunks for SW_MLEngineer_MahmoudSaeed.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_MahmoudSaeed.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_OmarMarie.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_OmarMarie.txt as plain text\n",
      "INFO:__main__:Created 14 initial chunks\n",
      "INFO:__main__:Created 14 chunks for SW_MLEngineer_OmarMarie.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_OmarMarie.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_OsamaFayez.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_OsamaFayez.txt as plain text\n",
      "INFO:__main__:Created 8 initial chunks\n",
      "INFO:__main__:Created 8 chunks for SW_MLEngineer_OsamaFayez.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_OsamaFayez.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_RohandaHamed.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_RohandaHamed.txt as plain text\n",
      "INFO:__main__:Created 6 initial chunks\n",
      "INFO:__main__:Created 6 chunks for SW_MLEngineer_RohandaHamed.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_RohandaHamed.txt\n",
      "INFO:__main__:Processing file: SW_MLEngineer_YoussefMedhat.txt\n",
      "INFO:__main__:Loaded SW_MLEngineer_YoussefMedhat.txt as plain text\n",
      "INFO:__main__:Created 4 initial chunks\n",
      "INFO:__main__:Created 4 chunks for SW_MLEngineer_YoussefMedhat.txt\n",
      "INFO:__main__:Saved chunks for SW_MLEngineer_YoussefMedhat.txt\n",
      "INFO:__main__:Saved combined chunks to CVs\\chunked_output\\all_chunks.json\n",
      "INFO:__main__:Saved chunks to CSV: CVs\\chunked_output\\all_chunks.csv\n",
      "INFO:__main__:Processing complete. Processed 23 files, created 408 total chunks\n",
      "INFO:__main__:Completed processing with 408 total chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_folder = \"CVs\\output\"\n",
    "output_folder = \"CVs\\chunked_output\"\n",
    "\n",
    "# check if the input folder exists and has files\n",
    "if not os.path.exists(input_folder):\n",
    "    logger.error(f\"Input folder '{input_folder}' does not exist!\")\n",
    "else:\n",
    "    files = os.listdir(input_folder)\n",
    "    if not files:\n",
    "        logger.error(f\"Input folder '{input_folder}' is empty!\")\n",
    "    else:\n",
    "        logger.info(f\"Found {len(files)} files in {input_folder}\")\n",
    "        \n",
    "        # Test with first file\n",
    "        first_file = os.path.join(input_folder, files[0])\n",
    "        logger.info(f\"Testing with first file: {first_file}\")\n",
    "        test_chunks = test_single_file(first_file, output_folder)\n",
    "        \n",
    "        if test_chunks:\n",
    "            # If test successful, process all files\n",
    "            logger.info(\"Processing all files...\")\n",
    "            total_chunks = process_and_save_chunks(input_folder, output_folder)\n",
    "            logger.info(f\"Completed processing with {total_chunks} total chunks\")\n",
    "        else:\n",
    "            logger.error(\"Test processing failed, please check the errors above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Headway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
